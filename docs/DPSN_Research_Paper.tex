\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{authblk}

\geometry{
    a4paper,
    total={170mm,257mm},
    left=20mm,
    top=20mm,
}

\title{Dynamic Parameter Selection Networks (DPSN): Ultra-Fine-Grained Sparse Activation for Neural Computation}

\author{Dev Kumar}
\affil{Independent Researcher}
\affil{\textit{devkumar@theorionic.com}}

\date{December 2025}

\begin{document}

\maketitle

\begin{abstract}
Modern Large Language Models (LLMs) operate on a dense activation paradigm where all parameters are utilized for every input token, or a coarse-grained sparse paradigm (Mixture of Experts) where large blocks of parameters are activated. This results in significant computational redundancy, as simple tokens (e.g., punctuation, stop words) consume the same compute budget as complex semantic concepts. We introduce \textbf{Dynamic Parameter Selection Networks (DPSN)}, a novel neural architecture that decouples parameter storage from computation. DPSN utilizes a massive, addressable \textbf{Parameter Pool} and a lightweight \textbf{Router} that dynamically selects a variable number of individual parameters—from hundreds to thousands—based on input complexity. This approach enables ultra-fine-grained sparsity at the individual weight level, allowing the model to construct a transient, optimal computation graph for each token. We demonstrate that DPSN can successfully learn to route inputs to specific parameter subsets and update knowledge sparsely, offering a path toward models that scale parameter count into the billions while maintaining constant-time inference costs proportional only to task complexity.

\textbf{Keywords:} Sparse Neural Networks, Conditional Computation, Dynamic Routing, Adaptive Compute, Large Language Models.
\end{abstract}

\section{Introduction}

The scaling laws of deep learning have driven parameter counts into the trillions, yet the computational paradigm remains largely static: for a given layer, a matrix multiplication involves every weight in that matrix. While \textbf{Mixture of Experts (MoE)} architectures have successfully introduced conditional computation, they operate at a "macro" level—routing inputs to one of $N$ large feed-forward networks (Experts). This granularity restricts the model's flexibility; it must pick an entire "Expert" or nothing.

We propose a shift to "micro" granularity. \textbf{Dynamic Parameter Selection Networks (DPSN)} treat model parameters not as fixed entries in a matrix, but as a disjoint \textbf{Memory Pool} of knowledge. A router dynamically retrieves specific weights from this pool to construct a temporary weight matrix on-the-fly.

This architecture offers three primary contributions:
\begin{enumerate}
    \item \textbf{Parameter-Level Granularity:} Unlike MoE, which routes to experts of millions of parameters, DPSN routes to individual parameters or small groups, enabling highly specific feature combinations.
    \item \textbf{Adaptive Compute Budget:} The model predicts the "complexity" of the input token and dynamically adjusts the number of active parameters. A newline character might trigger 100 parameters; a complex concept might trigger 5,000.
    \item \textbf{Disjoint Training:} We demonstrate a training methodology where gradients flow only to the selected parameters, ensuring that the vast majority of the Parameter Pool remains frozen (sparse updates), mitigating catastrophic forgetting and enabling vast scalability.
\end{enumerate}

\section{Methodology}

The DPSN architecture consists of three distinct components: the \textbf{Route Generator (Router)}, the \textbf{Parameter Pool}, and the \textbf{Sparse Execution Engine}.

\subsection{The Parameter Pool}
The Pool $P$ is a learnable matrix of size $M \times D$, where $M$ is the total memory size (e.g., 100,000+) and $D$ is the parameter dimension. Unlike standard layers, these parameters have no fixed spatial position in the computation graph until selected.

$$ P \in \mathbb{R}^{M \times D} $$

\subsection{The Route Generator (Router)}
The Router $R(x)$ serves as the "Librarian." It takes the input token embedding $x$ and outputs two signals: a \textbf{Budget} $k$ and a set of \textbf{Indices} $I$.

\begin{enumerate}
    \item \textbf{Complexity Analysis:} A lightweight network estimates the difficulty of the input scalar $c \in [0, 1]$.
    $$ c = \sigma(W_c x + b_c) $$
    The budget $k$ is determined dynamically:
    $$ k = \lfloor k_{min} + (k_{max} - k_{min}) \cdot c^2 \rfloor $$

    \item \textbf{Index Selection:} The router predicts a relevance score $S$ for all $M$ parameters in the pool.
    $$ S = \text{ReLU}(W_1 x) W_2 $$
    To select indices $I$, we select the top-$k$ scores. During training, we inject noise to encourage exploration of the pool:
    $$ I = \text{TopK}(S + \epsilon, k) $$
\end{enumerate}

\subsection{The Sparse Execution Engine}
The Engine performs the actual computation using only the retrieval parameters.

\begin{enumerate}
    \item \textbf{Retrieval:} We fetch the rows from $P$ corresponding to indices $I$.
    $$ W_{active} = P[I] \in \mathbb{R}^{k \times D} $$

    \item \textbf{Dynamic Projection:} The input $x$ is projected onto these selected weights. This is functionally equivalent to a dynamic linear layer where the weights change for every sample.
    $$ y = x W_{active}^T $$

    \item \textbf{Aggregation:} The results are weighted by the router's softmax probability (to ensure differentiability w.r.t the router) and aggregated back to the model dimension.
\end{enumerate}

\section{Training Methodology}

DPSN employs a \textbf{Joint Sparse Training} strategy. The objective function $\mathcal{L}$ minimizes the prediction error (e.g., Cross-Entropy for Language Modeling).

\subsection{Sparse Gradient Flow}
During backpropagation, gradients $\nabla \mathcal{L}$ flow through the Execution Engine back to the selected parameters $P[I]$ and the Router.

\begin{itemize}
    \item \textbf{Pool Updates:} Only the row vectors in $P$ indexed by $I$ receive non-zero gradients. If $|I| = 500$ and $M = 20,000$, then 97.5\% of the pool receives exactly zero gradients. This allows the pool to act as a long-term memory store where unused knowledge is preserved.
    \item \textbf{Router Updates:} The router receives gradients based on the performance of the parameters it selected, learning to map specific input features to specific indices in the pool.
\end{itemize}

\section{Experiments and Results}

We validated the DPSN architecture on the \textbf{Tiny Shakespeare} dataset to demonstrate convergence and dynamic behavior.

\subsection{Experimental Setup}
\begin{itemize}
    \item \textbf{Pool Size:} 20,000 slots (1.28 Million parameters total).
    \item \textbf{Embedding Dimension:} 64.
    \item \textbf{Budget Range:} 100 to 5,000 parameters.
    \item \textbf{Training Steps:} 500 iterations (Proof of Concept).
\end{itemize}

\subsection{Dynamic Behavior Analysis}
We analyzed the generation process token-by-token to verify the adaptive budget mechanism.

\begin{table}[h]
    \centering
    \begin{tabular}{lccc}
        \toprule
        Token & Complexity Score & Budget Used & Inference Time \\
        \midrule
        \texttt{a} & 0.51 & $\sim$1400 & 2.1 ms \\
        \texttt{\textbackslash n} (newline) & 0.54 & $\sim$1530 & 3.3 ms \\
        \texttt{the} & 0.52 & $\sim$1450 & 2.5 ms \\
        \bottomrule
    \end{tabular}
    \caption{Dynamic Budget Allocation per Token}
    \label{tab:dynamic_budget}
\end{table}

The model successfully identified the newline character \texttt{\textbackslash n} as a higher-complexity token (likely due to the context reset implications), automatically allocating $\sim$10\% more parameters to process it compared to a standard character.

\subsection{Learning Curve}
Despite the highly stochastic nature of parameter selection in the early phases, the model demonstrated rapid convergence.
\begin{itemize}
    \item \textbf{Step 0:} Random noise generation.
    \item \textbf{Step 50:} Emergence of word structure (spaces, common bigrams like "th", "he").
    \item \textbf{Step 200:} Significant loss reduction from 4.33 to 2.67.
\end{itemize}

\section{Discussion}

\subsection{Granularity vs. MoE}
Standard MoE models route tokens to experts which are essentially full Neural Networks. DPSN routes tokens to \textit{weights}. This allows DPSN to compose "Experts" on the fly. If an MoE has 8 experts, it can only produce 8 types of processing (or combinations thereof). A DPSN with 100,000 parameters selecting 1,000 can theoretically produce $\binom{100,000}{1,000}$ unique combinations of weights, offering a combinatorial explosion of representational capacity.

\subsection{The "Librarian" Metaphor}
The system behaves akin to a librarian (Router) and a library (Pool). As the library grows to billions of parameters, the search time remains constant (or logarithmic if hierarchical routing is used), while the "reading" time (Executor) depends only on the books selected, not the library size. This decoupling is key to breaking the memory-compute linear relationship in standard LLMs.

\section{Conclusion}

Dynamic Parameter Selection Networks represent a viable path toward efficient, ultra-large-scale neural networks. By proving that a model can learn to dynamically assemble its own weight matrices from a massive, disjoint pool, we open new avenues for "Life-Long Learning" models where the parameter pool can grow indefinitely without increasing inference latency.

Future work will focus on hierarchical routing mechanisms to scale the Parameter Pool to billions of entries and investigating the semantic clustering of parameters within the pool.

\end{document}
